<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Apache Spark Gotchas | enamya | blog</title><meta name=keywords content="apache,spark,gotchas,tips,big data,data"><meta name=description content="Apache Spark is an open-source, distributed computing system that provides a wide range of tools for data processing, analytics, and machine learning. It&rsquo;s a popular choice for many organizations due to its ability to scale and its support for a wide range of programming languages. However, like any complex system, there are a few gotchas that users should be aware of when working with Spark. In this post, I will cover some of the most common gotchas that I learned while working with Spark, it will also be a sort of summary of some concepts I read about in the Learning Spark: Lightning-Fast Big Data Analysis book (link)."><meta name=author content="Aymane BOUMAAZA"><link rel=canonical href=https://aymane11.github.io/posts/apache-spark-gotchas/><link crossorigin=anonymous href=/assets/css/stylesheet.min.ec8da366ca2fb647537ccb7a8f6fa5b4e9cd3c7a0d3171dd2d3baad1e49c8bfc.css integrity="sha256-7I2jZsovtkdTfMt6j2+ltOnNPHoNMXHdLTuq0eSci/w=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://aymane11.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://aymane11.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://aymane11.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://aymane11.github.io/apple-touch-icon.png><link rel=mask-icon href=https://aymane11.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Apache Spark Gotchas"><meta property="og:description" content="Apache Spark is an open-source, distributed computing system that provides a wide range of tools for data processing, analytics, and machine learning. It&rsquo;s a popular choice for many organizations due to its ability to scale and its support for a wide range of programming languages. However, like any complex system, there are a few gotchas that users should be aware of when working with Spark. In this post, I will cover some of the most common gotchas that I learned while working with Spark, it will also be a sort of summary of some concepts I read about in the Learning Spark: Lightning-Fast Big Data Analysis book (link)."><meta property="og:type" content="article"><meta property="og:url" content="https://aymane11.github.io/posts/apache-spark-gotchas/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-18T18:19:19+01:00"><meta property="article:modified_time" content="2023-01-18T18:19:19+01:00"><meta property="og:site_name" content="enamya blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Apache Spark Gotchas"><meta name=twitter:description content="Apache Spark is an open-source, distributed computing system that provides a wide range of tools for data processing, analytics, and machine learning. It&rsquo;s a popular choice for many organizations due to its ability to scale and its support for a wide range of programming languages. However, like any complex system, there are a few gotchas that users should be aware of when working with Spark. In this post, I will cover some of the most common gotchas that I learned while working with Spark, it will also be a sort of summary of some concepts I read about in the Learning Spark: Lightning-Fast Big Data Analysis book (link)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://aymane11.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Apache Spark Gotchas","item":"https://aymane11.github.io/posts/apache-spark-gotchas/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Spark Gotchas","name":"Apache Spark Gotchas","description":"Apache Spark is an open-source, distributed computing system that provides a wide range of tools for data processing, analytics, and machine learning. It\u0026rsquo;s a popular choice for many organizations due to its ability to scale and its support for a wide range of programming languages. However, like any complex system, there are a few gotchas that users should be aware of when working with Spark. In this post, I will cover some of the most common gotchas that I learned while working with Spark, it will also be a sort of summary of some concepts I read about in the Learning Spark: Lightning-Fast Big Data Analysis book (link).","keywords":["apache","spark","gotchas","tips","big data","data"],"articleBody":"Apache Spark is an open-source, distributed computing system that provides a wide range of tools for data processing, analytics, and machine learning. Itâ€™s a popular choice for many organizations due to its ability to scale and its support for a wide range of programming languages. However, like any complex system, there are a few gotchas that users should be aware of when working with Spark. In this post, I will cover some of the most common gotchas that I learned while working with Spark, it will also be a sort of summary of some concepts I read about in the Learning Spark: Lightning-Fast Big Data Analysis book (link).\nLazy Evaluation Spark uses a technique called lazy evaluation to optimize the processing of RDDs. This means that transformations on RDDs are not actually executed until an action (e.g., show, count, collect, take) is called on them. This can lead to confusion, as it is not always clear when a transformation will be executed. For example, when timing a transformation, it is important to note that the transformation will not be executed until an action is called, so there is no point in timing the transformation instruction itself.\nval bigDf = ... val transformed = bigDf.filter(...).map(...) // this transformation will not be executed yet (lazy evaluation) transformed.count() // this action will trigger the execution of the transformation and will take more time than the transformation itself Joins Join strategies There are two main strategies for performing joins in Spark: broadcast joins and shuffle joins. The strategy used depends (not only) on the size of the dataframes being joined, and Spark will automatically choose the best strategy. However, it is important to understand how each strategy works to avoid unexpected behavior.\nBroadcast joins Broadcast joins are used when one of the dataframes being joined is small enough to fit in memory (10MB by default). Spark will send a copy of the small dataframe to all nodes in the cluster (hence the name broadast join), and then perform the join locally on each node. This is generally faster than a shuffle join, but it can lead to out of memory errors if the dataframe is too large. Shuffle joins In shuffle joins, Spark will shuffle the dataframes partitions between nodes, to ensure that all data with the same key is on the same node. This is generally slower than a broadcast join because of all the shuffling that occurs, but it can be used to join dataframes of any size. See: Spark Join Strategies - How \u0026 What? and The art of joining in Spark\nDuplicate columns after joins An issue that I faced multiple times when working with Spark is having duplicate columns after performing a join between two dataframes having the same join column name.\nval left = Seq((\"John\", \"Doe\", 29), (\"Aymane\", \"Boumaaza\", 20), (\"Jane\", \"Doe\", 29) ).toDF(\"firstname\", \"lastname\", \"age\") // +---------+--------+---+ // |firstname|lastname|age| // +---------+--------+---+ // | John| Doe| 29| // | Aymane|Boumaaza| 20| // | Jane| Doe| 29| // +---------+--------+---+ val right = Seq((\"John\"), (\"Aymane\"), (\"Jane\")).toDF(\"firstname\") // +---------+ // |firstname| // +---------+ // | John| // | Aymane| // | Jane| // +---------+ val joined = left.join(right, left(\"firstname\") === right(\"firstname\")) // +---------+--------+---+---------+ // |firstname|lastname|age|firstname| // Notice the duplicate column (firstname) // +---------+--------+---+---------+ // | John| Doe| 29| John| // | Aymane|Boumaaza| 20| Aymane| // | Jane| Doe| 29| Jane| // +---------+--------+---+---------+ When performing a join in Spark, use a list/sequence as the by parameter to avoid duplicate columns to ensure that the resulting dataframe only has a single copy of each column.\nval joined = left.join(right, Seq(\"firstname\")) // or val joined = left.join(right, \"firstname\") if using only one column to join // +---------+--------+---+ // |firstname|lastname|age| // +---------+--------+---+ // | John| Doe| 29| // | Aymane|Boumaaza| 20| // | Jane| Doe| 29| // +---------+--------+---+ See: Databricks - Prevent duplicated columns when joining two DataFrames\nWriting to files Filenames When writing to file in Spark, it is important to note that the output will be written to a directory, not a single file. This is because each partition is written to a separate file, and the files are combined into a single directory. It is important to keep this in mind when specifying the output path, as adding a .csv extension to the path will create a directory with the name filename.csv rather than a single file, therefore thereâ€™s no need to add the extension (since itâ€™s a directory).\nval df = Seq((\"John\", \"Doe\", \"29\"), (\"Aymane\", \"Boumaaza\", \"20\"), (\"Jane\", \"Doe\", \"29\") ).toDF(\"firstname\", \"lastname\", \"age\") df.write.csv(\"/tmp/my_csv.csv\") df.write.parquet(\"/tmp/my_parquet.parquet\") /tmp/my_csv.csv and /tmp/my_parquet.parquet will be both directories containing the actual csv and parquet files.\n$ ls -l /tmp drwxr-xr-x 2 root root 4096 Jan 4 20:20 my_csv.csv drwxr-xr-x 2 root root 4096 Jan 4 20:20 my_parquet.parquet Number of partitions When writing to file in Apache Spark, itâ€™s important to consider the number of partitions in your data. The more partitions you have, the more files are created. This can be beneficial if you are working with a large dataset and want to parallelize the writing process. However, if you have a small dataset, using too many partitions can lead to unnecessary overhead and slower performance, and vice versa when working with large dataframes and having small number of partitions will result in less files but bigger ones. It is important to carefully consider the size and structure of your dataset when deciding on the number of partitions to use.\nSmall DataFrame Minimize the number of partitions to avoid unnecessary overhead.\nval smallDf = Seq((\"John\", \"Doe\", \"29\"), (\"Aymane\", \"Boumaaza\", \"20\"), (\"Jane\", \"Doe\", \"29\") ).toDF(\"firstname\", \"lastname\", \"age\") smallDf.rdd.partitions.size // Int = 3 smallDf.write.csv(\"/tmp/output\") // /tmp/output will be a directory containing 3 (number of partitions) csv files smallDf.coalesce(1).write.csv(\"/tmp/output\") // or repartition(1) // /tmp/output will be a directory containing a single csv file (with other files) Big DataFrame Use a reasonable number of partitions to have manageable output.\nval bigDf = ... bigDf.rdd.partitions.size // Int = 200 bigDf.write.csv(\"/tmp/output\") // /tmp/output.csv will be a directory containing 200 (number of partitions) csv files bigDf.coalesce(10).write.csv(\"/tmp/output\") // or repartition(10) // /tmp/output.csv will be a directory containing 10 csv files which is more manageable than 200 files Write Options When saving a dataframe to file in Spark, it is important to consider the various options available to you. For example, the header option controls whether the column names are included in the output csv file. By default, this option is set to false, which means that header containing the column names will not be included. It is important to carefully consider the options available to you when writing to file to ensure that the output is in the desired format.\nval df = Seq((\"John\", \"Doe\", \"29\"), (\"Aymane\", \"Boumaaza\", \"20\"), (\"Jane\", \"Doe\", \"29\") ).toDF(\"firstname\", \"lastname\", \"age\") df.write.csv(\"/tmp/output\") // header is false by default df.write.option(\"header\",true).option(\"delimiter\",\"|\").csv(\"/tmp/output\") See Spark SQL - Data Source\nPySpark UDFs vs Pandas UDFs When working with User-Defined Functions (UDFs) in PySpark (2.3+), it is often more efficient to use Pandas UDFs (also known as vectorized UDFs) instead of standard Spark UDFs. Because PySpark UDFs require data movement between the JVM and Python, which is expensive, Pandas UDFs allow you to process the data using the power of vectorization and Apache Arrow. This can significantly improve the performance of your Spark application.\nimport pandas as pd from pyspark.sql.types import StringType from pyspark.sql.functions import pandas_udf, udf df = spark.createDataFrame( data=[ (\"Aymane\", \"Boumaaza\", 20), (\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 40), ], schema=[\"firstname\", \"lastname\", \"age\"], ) # Standard UDF @udf(returnType=StringType()) def fullname(firstname, lastname): return f\"{firstname.capitalize()} {lastname.upper()}\" df.withColumn(\"fullname\", fullname(\"firstname\", \"lastname\")) # Pandas UDF @pandas_udf(StringType()) def fullname_pd(firstname: pd.Series, lastname: pd.Series) -\u003e pd.Series: # You can use any of the pandas APIs here return firstname.str.capitalize() + \" \" + lastname.str.upper() df.withColumn(\"fullname\", fullname_pd(\"firstname\", \"lastname\")) cache vs persist In Spark, you can use the cache and persist functions to store data in memory for faster access. However, there are some important differences between these two functions. The persist function allows you to specify the storage level (e.g., memory only, memory and disk, etc.), while the cache function stores data in the default storage level MEMORY_AND_DISK, first Spark tries to store the dataframe in memory, if thereâ€™s any excess, it will be stored in disk. It is important to choose the appropriate function based on your use case. The choice of storage level is crucial as it can have a significant impact on the performance of your Spark application.\nNote from the Learning Spark: Lightning-Fast Big Data Analysis book:\nWhen you use cache() or persist(), the DataFrame is not fully cached until you invoke an action that goes through every record (e.g., count()). If you use an action like take(1), only one partition will be cached because Catalyst realizes that you do not need to compute all the partitions just to retrieve one record.\nConclusion Most of the topics I covered in this article were introduced to me in the Learning Spark: Lightning-Fast Big Data Analysis book, that I highly recommend to anyone who wants to learn more about Spark, itâ€™s a great resource to know more about Spark. I hope you found this article useful, if you have any questions or comments, please reach out. Thanks for reading!\n","wordCount":"1525","inLanguage":"en","datePublished":"2023-01-18T18:19:19+01:00","dateModified":"2023-01-18T18:19:19+01:00","author":{"@type":"Person","name":"Aymane BOUMAAZA"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://aymane11.github.io/posts/apache-spark-gotchas/"},"publisher":{"@type":"Organization","name":"enamya | blog","logo":{"@type":"ImageObject","url":"https://aymane11.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://aymane11.github.io/ accesskey=h title="enamya | blog (Alt + H)">enamya | blog</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://www.enamya.me/ title=Portfolio><span>Portfolio</span></a></li><li><a href=https://aymane11.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://aymane11.github.io/tags/til/ title=til><span>til</span></a></li><li><a href=https://aymane11.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://aymane11.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aymane11.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://aymane11.github.io/posts/>Posts</a></div><h1 class=post-title>Apache Spark Gotchas</h1><div class=post-meta><span title='2023-01-18 18:19:19 +0100 +0100'>January 18, 2023</span>&nbsp;Â·&nbsp;8 min&nbsp;Â·&nbsp;Aymane BOUMAAZA&nbsp;|&nbsp;<a href=https://github.com/Aymane11/enamya-blog/blob/main/content/posts/apache-spark-gotchas.md rel="noopener noreferrer" target=_blank>View on GitHub</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#lazy-evaluation aria-label="Lazy Evaluation">Lazy Evaluation</a></li><li><a href=#joins aria-label=Joins>Joins</a><ul><li><a href=#join-strategies aria-label="Join strategies">Join strategies</a><ul><li><a href=#broadcast-joins aria-label="Broadcast joins">Broadcast joins</a></li><li><a href=#shuffle-joins aria-label="Shuffle joins">Shuffle joins</a></li></ul></li><li><a href=#duplicate-columns-after-joins aria-label="Duplicate columns after joins">Duplicate columns after joins</a></li></ul></li><li><a href=#writing-to-files aria-label="Writing to files">Writing to files</a><ul><li><a href=#filenames aria-label=Filenames>Filenames</a></li><li><a href=#number-of-partitions aria-label="Number of partitions">Number of partitions</a><ul><li><a href=#small-dataframe aria-label="Small DataFrame">Small DataFrame</a></li><li><a href=#big-dataframe aria-label="Big DataFrame">Big DataFrame</a></li></ul></li><li><a href=#write-options aria-label="Write Options">Write Options</a></li></ul></li><li><a href=#pyspark-udfs-vs-pandas-udfs aria-label="PySpark UDFs vs Pandas UDFs">PySpark UDFs vs Pandas UDFs</a></li><li><a href=#cache-vs-persist aria-label="cache vs persist"><code>cache</code> vs <code>persist</code></a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>Apache Spark is an open-source, distributed computing system that provides a wide range of tools for data processing, analytics, and machine learning. It&rsquo;s a popular choice for many organizations due to its ability to scale and its support for a wide range of programming languages. However, like any complex system, there are a few gotchas that users should be aware of when working with Spark. In this post, I will cover some of the most common gotchas that I learned while working with Spark, it will also be a sort of summary of some concepts I read about in the <strong>Learning Spark: Lightning-Fast Big Data Analysis book</strong> (<strong><a href=https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf>link</a></strong>).</p><h1 id=lazy-evaluation>Lazy Evaluation<a hidden class=anchor aria-hidden=true href=#lazy-evaluation>#</a></h1><p>Spark uses a technique called <strong>lazy evaluation</strong> to optimize the processing of RDDs. This means that <a href=https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations>transformations</a> on RDDs are not actually executed until an <a href=https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions>action</a> (e.g., show, count, collect, take) is called on them. This can lead to confusion, as it is not always clear when a transformation will be executed. For example, when timing a transformation, it is important to note that the transformation will not be executed until an action is called, so there is no point in timing the transformation instruction itself.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>bigDf</span> <span class=k>=</span> <span class=o>...</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>transformed</span> <span class=k>=</span> <span class=n>bigDf</span><span class=o>.</span><span class=n>filter</span><span class=o>(...).</span><span class=n>map</span><span class=o>(...)</span> <span class=c1>// this transformation will not be executed yet (lazy evaluation)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>transformed</span><span class=o>.</span><span class=n>count</span><span class=o>()</span> <span class=c1>// this action will trigger the execution of the transformation and will take more time than the transformation itself
</span></span></span></code></pre></div><h1 id=joins>Joins<a hidden class=anchor aria-hidden=true href=#joins>#</a></h1><h2 id=join-strategies>Join strategies<a hidden class=anchor aria-hidden=true href=#join-strategies>#</a></h2><p>There are two main strategies for performing joins in Spark: broadcast joins and shuffle joins. The strategy used depends (not only) on the size of the dataframes being joined, and Spark will automatically choose the best strategy. However, it is important to understand how each strategy works to avoid unexpected behavior.</p><h3 id=broadcast-joins>Broadcast joins<a hidden class=anchor aria-hidden=true href=#broadcast-joins>#</a></h3><p>Broadcast joins are used when one of the dataframes being joined is small enough to fit in <strong>memory</strong> (<a href="https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options:~:text=1.3.0-,spark.sql.autoBroadcastJoinThreshold,-10485760%20(10%20MB)">10MB by default</a>). Spark will <strong>send a copy of the small dataframe to all nodes in the cluster</strong> (hence the name <strong>broadast</strong> join), and then perform the join locally on each node. This is generally faster than a shuffle join, but it can lead to out of memory errors if the dataframe is too large.
<img loading=lazy src=/broadcast_join.png alt="broadcast join"></p><h3 id=shuffle-joins>Shuffle joins<a hidden class=anchor aria-hidden=true href=#shuffle-joins>#</a></h3><p>In shuffle joins, Spark will <strong>shuffle the dataframes partitions between nodes</strong>, to ensure that all data with the same key is on the same node. This is generally slower than a broadcast join because of all the shuffling that occurs, but it can be used to join dataframes of any size.
<img loading=lazy src=/shuffle_join.png alt="shuffle join"></p><blockquote><p>See: <a href=https://towardsdatascience.com/strategies-of-spark-join-c0e7b4572bcf>Spark Join Strategies - How & What?</a> and <a href=https://towardsdatascience.com/the-art-of-joining-in-spark-dcbd33d693c>The art of joining in Spark</a></p></blockquote><h2 id=duplicate-columns-after-joins>Duplicate columns after joins<a hidden class=anchor aria-hidden=true href=#duplicate-columns-after-joins>#</a></h2><p>An issue that I faced multiple times when working with Spark is having duplicate columns after performing a join between two dataframes having the same join column name.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>left</span> <span class=k>=</span> <span class=nc>Seq</span><span class=o>((</span><span class=s>&#34;John&#34;</span><span class=o>,</span> <span class=s>&#34;Doe&#34;</span><span class=o>,</span> <span class=mi>29</span><span class=o>),</span>
</span></span><span class=line><span class=cl>            <span class=o>(</span><span class=s>&#34;Aymane&#34;</span><span class=o>,</span> <span class=s>&#34;Boumaaza&#34;</span><span class=o>,</span> <span class=mi>20</span><span class=o>),</span>
</span></span><span class=line><span class=cl>            <span class=o>(</span><span class=s>&#34;Jane&#34;</span><span class=o>,</span> <span class=s>&#34;Doe&#34;</span><span class=o>,</span> <span class=mi>29</span><span class=o>)</span>
</span></span><span class=line><span class=cl>        <span class=o>).</span><span class=n>toDF</span><span class=o>(</span><span class=s>&#34;firstname&#34;</span><span class=o>,</span> <span class=s>&#34;lastname&#34;</span><span class=o>,</span> <span class=s>&#34;age&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+
</span></span></span><span class=line><span class=cl><span class=c1>// |firstname|lastname|age|
</span></span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+
</span></span></span><span class=line><span class=cl><span class=c1>// |     John|     Doe| 29|
</span></span></span><span class=line><span class=cl><span class=c1>// |   Aymane|Boumaaza| 20|
</span></span></span><span class=line><span class=cl><span class=c1>// |     Jane|     Doe| 29|
</span></span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>right</span> <span class=k>=</span> <span class=nc>Seq</span><span class=o>((</span><span class=s>&#34;John&#34;</span><span class=o>),</span> <span class=o>(</span><span class=s>&#34;Aymane&#34;</span><span class=o>),</span> <span class=o>(</span><span class=s>&#34;Jane&#34;</span><span class=o>)).</span><span class=n>toDF</span><span class=o>(</span><span class=s>&#34;firstname&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// +---------+
</span></span></span><span class=line><span class=cl><span class=c1>// |firstname|
</span></span></span><span class=line><span class=cl><span class=c1>// +---------+
</span></span></span><span class=line><span class=cl><span class=c1>// |     John|
</span></span></span><span class=line><span class=cl><span class=c1>// |   Aymane|
</span></span></span><span class=line><span class=cl><span class=c1>// |     Jane|
</span></span></span><span class=line><span class=cl><span class=c1>// +---------+
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>joined</span> <span class=k>=</span> <span class=n>left</span><span class=o>.</span><span class=n>join</span><span class=o>(</span><span class=n>right</span><span class=o>,</span> <span class=n>left</span><span class=o>(</span><span class=s>&#34;firstname&#34;</span><span class=o>)</span> <span class=o>===</span> <span class=n>right</span><span class=o>(</span><span class=s>&#34;firstname&#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+---------+
</span></span></span><span class=line><span class=cl><span class=c1>// |firstname|lastname|age|firstname|   // Notice the duplicate column (firstname)
</span></span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+---------+
</span></span></span><span class=line><span class=cl><span class=c1>// |     John|     Doe| 29|     John|
</span></span></span><span class=line><span class=cl><span class=c1>// |   Aymane|Boumaaza| 20|   Aymane|
</span></span></span><span class=line><span class=cl><span class=c1>// |     Jane|     Doe| 29|     Jane|
</span></span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+---------+
</span></span></span></code></pre></div><p>When performing a join in Spark, use a list/sequence as the <code>by</code> parameter to avoid duplicate columns to ensure that the resulting dataframe only has a single copy of each column.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>joined</span> <span class=k>=</span> <span class=n>left</span><span class=o>.</span><span class=n>join</span><span class=o>(</span><span class=n>right</span><span class=o>,</span> <span class=nc>Seq</span><span class=o>(</span><span class=s>&#34;firstname&#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=c1>// or val joined = left.join(right, &#34;firstname&#34;) if using only one column to join
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+
</span></span></span><span class=line><span class=cl><span class=c1>// |firstname|lastname|age|
</span></span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+
</span></span></span><span class=line><span class=cl><span class=c1>// |     John|     Doe| 29|
</span></span></span><span class=line><span class=cl><span class=c1>// |   Aymane|Boumaaza| 20|
</span></span></span><span class=line><span class=cl><span class=c1>// |     Jane|     Doe| 29|
</span></span></span><span class=line><span class=cl><span class=c1>// +---------+--------+---+
</span></span></span></code></pre></div><blockquote><p>See: <a href=https://kb.databricks.com/en_US/data/join-two-dataframes-duplicated-columns>Databricks - Prevent duplicated columns when joining two DataFrames</a></p></blockquote><h1 id=writing-to-files>Writing to files<a hidden class=anchor aria-hidden=true href=#writing-to-files>#</a></h1><h2 id=filenames>Filenames<a hidden class=anchor aria-hidden=true href=#filenames>#</a></h2><p>When writing to file in Spark, it is important to note that the output will be written to a <strong>directory, not a single file</strong>. This is because each partition is written to a separate file, and the files are combined into a single directory. It is important to keep this in mind when specifying the output path, as adding a <code>.csv</code> extension to the path will create a directory with the name <code>filename.csv</code> rather than a single file, <strong>therefore there&rsquo;s no need to add the extension (since it&rsquo;s a directory)</strong>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>df</span> <span class=k>=</span> <span class=nc>Seq</span><span class=o>((</span><span class=s>&#34;John&#34;</span><span class=o>,</span> <span class=s>&#34;Doe&#34;</span><span class=o>,</span> <span class=s>&#34;29&#34;</span><span class=o>),</span>
</span></span><span class=line><span class=cl>            <span class=o>(</span><span class=s>&#34;Aymane&#34;</span><span class=o>,</span> <span class=s>&#34;Boumaaza&#34;</span><span class=o>,</span> <span class=s>&#34;20&#34;</span><span class=o>),</span>
</span></span><span class=line><span class=cl>            <span class=o>(</span><span class=s>&#34;Jane&#34;</span><span class=o>,</span> <span class=s>&#34;Doe&#34;</span><span class=o>,</span> <span class=s>&#34;29&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>        <span class=o>).</span><span class=n>toDF</span><span class=o>(</span><span class=s>&#34;firstname&#34;</span><span class=o>,</span> <span class=s>&#34;lastname&#34;</span><span class=o>,</span> <span class=s>&#34;age&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>csv</span><span class=o>(</span><span class=s>&#34;/tmp/my_csv.csv&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>parquet</span><span class=o>(</span><span class=s>&#34;/tmp/my_parquet.parquet&#34;</span><span class=o>)</span>
</span></span></code></pre></div><p><code>/tmp/my_csv.csv</code> and <code>/tmp/my_parquet.parquet</code> will be both <strong>directories</strong> containing the actual <code>csv</code> and <code>parquet</code> files.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-console data-lang=console><span class=line><span class=cl><span class=gp>$</span> ls -l /tmp
</span></span><span class=line><span class=cl><span class=go>drwxr-xr-x  2 root root   4096 Jan  4 20:20 my_csv.csv
</span></span></span><span class=line><span class=cl><span class=go>drwxr-xr-x  2 root root   4096 Jan  4 20:20 my_parquet.parquet
</span></span></span></code></pre></div><h2 id=number-of-partitions>Number of partitions<a hidden class=anchor aria-hidden=true href=#number-of-partitions>#</a></h2><p>When writing to file in Apache Spark, it&rsquo;s important to consider the number of partitions in your data. <strong>The more partitions you have, the more files are created</strong>. This can be beneficial if you are working with a large dataset and want to parallelize the writing process. However, if you have a small dataset, using too many partitions can lead to unnecessary overhead and slower performance, and vice versa when working with large dataframes and having small number of partitions will result in less files but bigger ones. It is important to carefully consider the size and structure of your dataset when deciding on the number of partitions to use.</p><h3 id=small-dataframe>Small DataFrame<a hidden class=anchor aria-hidden=true href=#small-dataframe>#</a></h3><p>Minimize the number of partitions to avoid unnecessary overhead.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>smallDf</span> <span class=k>=</span> <span class=nc>Seq</span><span class=o>((</span><span class=s>&#34;John&#34;</span><span class=o>,</span> <span class=s>&#34;Doe&#34;</span><span class=o>,</span> <span class=s>&#34;29&#34;</span><span class=o>),</span>
</span></span><span class=line><span class=cl>            <span class=o>(</span><span class=s>&#34;Aymane&#34;</span><span class=o>,</span> <span class=s>&#34;Boumaaza&#34;</span><span class=o>,</span> <span class=s>&#34;20&#34;</span><span class=o>),</span>
</span></span><span class=line><span class=cl>            <span class=o>(</span><span class=s>&#34;Jane&#34;</span><span class=o>,</span> <span class=s>&#34;Doe&#34;</span><span class=o>,</span> <span class=s>&#34;29&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>        <span class=o>).</span><span class=n>toDF</span><span class=o>(</span><span class=s>&#34;firstname&#34;</span><span class=o>,</span> <span class=s>&#34;lastname&#34;</span><span class=o>,</span> <span class=s>&#34;age&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>smallDf</span><span class=o>.</span><span class=n>rdd</span><span class=o>.</span><span class=n>partitions</span><span class=o>.</span><span class=n>size</span> <span class=c1>// Int = 3
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=n>smallDf</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>csv</span><span class=o>(</span><span class=s>&#34;/tmp/output&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=c1>// /tmp/output will be a directory containing 3 (number of partitions) csv files
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=n>smallDf</span><span class=o>.</span><span class=n>coalesce</span><span class=o>(</span><span class=mi>1</span><span class=o>).</span><span class=n>write</span><span class=o>.</span><span class=n>csv</span><span class=o>(</span><span class=s>&#34;/tmp/output&#34;</span><span class=o>)</span> <span class=c1>// or repartition(1)
</span></span></span><span class=line><span class=cl><span class=c1>// /tmp/output will be a directory containing a single csv file (with other files)
</span></span></span></code></pre></div><h3 id=big-dataframe>Big DataFrame<a hidden class=anchor aria-hidden=true href=#big-dataframe>#</a></h3><p>Use a reasonable number of partitions to have manageable output.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>bigDf</span> <span class=k>=</span> <span class=o>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>bigDf</span><span class=o>.</span><span class=n>rdd</span><span class=o>.</span><span class=n>partitions</span><span class=o>.</span><span class=n>size</span> <span class=c1>// Int = 200
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=n>bigDf</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>csv</span><span class=o>(</span><span class=s>&#34;/tmp/output&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=c1>// /tmp/output.csv will be a directory containing 200 (number of partitions) csv files
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=n>bigDf</span><span class=o>.</span><span class=n>coalesce</span><span class=o>(</span><span class=mi>10</span><span class=o>).</span><span class=n>write</span><span class=o>.</span><span class=n>csv</span><span class=o>(</span><span class=s>&#34;/tmp/output&#34;</span><span class=o>)</span> <span class=c1>// or repartition(10)
</span></span></span><span class=line><span class=cl><span class=c1>// /tmp/output.csv will be a directory containing 10 csv files which is more manageable than 200 files
</span></span></span></code></pre></div><h2 id=write-options>Write Options<a hidden class=anchor aria-hidden=true href=#write-options>#</a></h2><p>When saving a dataframe to file in Spark, it is important to consider the various options available to you. For example, the <code>header</code> option controls whether the column names are included in the output <code>csv</code> file. By default, this option is set to <code>false</code>, which means that header containing the column names will not be included. It is important to carefully consider the options available to you when writing to file to ensure that the output is in the desired format.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>df</span> <span class=k>=</span> <span class=nc>Seq</span><span class=o>((</span><span class=s>&#34;John&#34;</span><span class=o>,</span> <span class=s>&#34;Doe&#34;</span><span class=o>,</span> <span class=s>&#34;29&#34;</span><span class=o>),</span>
</span></span><span class=line><span class=cl>            <span class=o>(</span><span class=s>&#34;Aymane&#34;</span><span class=o>,</span> <span class=s>&#34;Boumaaza&#34;</span><span class=o>,</span> <span class=s>&#34;20&#34;</span><span class=o>),</span>
</span></span><span class=line><span class=cl>            <span class=o>(</span><span class=s>&#34;Jane&#34;</span><span class=o>,</span> <span class=s>&#34;Doe&#34;</span><span class=o>,</span> <span class=s>&#34;29&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>        <span class=o>).</span><span class=n>toDF</span><span class=o>(</span><span class=s>&#34;firstname&#34;</span><span class=o>,</span> <span class=s>&#34;lastname&#34;</span><span class=o>,</span> <span class=s>&#34;age&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>csv</span><span class=o>(</span><span class=s>&#34;/tmp/output&#34;</span><span class=o>)</span> <span class=c1>// header is false by default
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>write</span><span class=o>.</span><span class=n>option</span><span class=o>(</span><span class=s>&#34;header&#34;</span><span class=o>,</span><span class=kc>true</span><span class=o>).</span><span class=n>option</span><span class=o>(</span><span class=s>&#34;delimiter&#34;</span><span class=o>,</span><span class=s>&#34;|&#34;</span><span class=o>).</span><span class=n>csv</span><span class=o>(</span><span class=s>&#34;/tmp/output&#34;</span><span class=o>)</span>
</span></span></code></pre></div><blockquote><p>See <a href=https://spark.apache.org/docs/latest/sql-data-sources.html>Spark SQL - Data Source</a></p></blockquote><h1 id=pyspark-udfs-vs-pandas-udfs>PySpark UDFs vs Pandas UDFs<a hidden class=anchor aria-hidden=true href=#pyspark-udfs-vs-pandas-udfs>#</a></h1><p>When working with User-Defined Functions (UDFs) in <strong>PySpark</strong> (2.3+), it is often more efficient to use Pandas UDFs (also known as vectorized UDFs) instead of standard Spark UDFs. Because PySpark UDFs require data movement between the JVM and Python, which is expensive, Pandas UDFs allow you to process the data using the power of vectorization and Apache Arrow. This can significantly improve the performance of your Spark application.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.types</span> <span class=kn>import</span> <span class=n>StringType</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>pyspark.sql.functions</span> <span class=kn>import</span> <span class=n>pandas_udf</span><span class=p>,</span> <span class=n>udf</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>spark</span><span class=o>.</span><span class=n>createDataFrame</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=s2>&#34;Aymane&#34;</span><span class=p>,</span> <span class=s2>&#34;Boumaaza&#34;</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=s2>&#34;John&#34;</span><span class=p>,</span> <span class=s2>&#34;Doe&#34;</span><span class=p>,</span> <span class=mi>30</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=p>(</span><span class=s2>&#34;Jane&#34;</span><span class=p>,</span> <span class=s2>&#34;Doe&#34;</span><span class=p>,</span> <span class=mi>40</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>schema</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;firstname&#34;</span><span class=p>,</span> <span class=s2>&#34;lastname&#34;</span><span class=p>,</span> <span class=s2>&#34;age&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Standard UDF</span>
</span></span><span class=line><span class=cl><span class=nd>@udf</span><span class=p>(</span><span class=n>returnType</span><span class=o>=</span><span class=n>StringType</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>fullname</span><span class=p>(</span><span class=n>firstname</span><span class=p>,</span> <span class=n>lastname</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>firstname</span><span class=o>.</span><span class=n>capitalize</span><span class=p>()</span><span class=si>}</span><span class=s2> </span><span class=si>{</span><span class=n>lastname</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;fullname&#34;</span><span class=p>,</span> <span class=n>fullname</span><span class=p>(</span><span class=s2>&#34;firstname&#34;</span><span class=p>,</span> <span class=s2>&#34;lastname&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Pandas UDF</span>
</span></span><span class=line><span class=cl><span class=nd>@pandas_udf</span><span class=p>(</span><span class=n>StringType</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>fullname_pd</span><span class=p>(</span><span class=n>firstname</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>,</span> <span class=n>lastname</span><span class=p>:</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>pd</span><span class=o>.</span><span class=n>Series</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># You can use any of the pandas APIs here</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>firstname</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>capitalize</span><span class=p>()</span> <span class=o>+</span> <span class=s2>&#34; &#34;</span> <span class=o>+</span> <span class=n>lastname</span><span class=o>.</span><span class=n>str</span><span class=o>.</span><span class=n>upper</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>withColumn</span><span class=p>(</span><span class=s2>&#34;fullname&#34;</span><span class=p>,</span> <span class=n>fullname_pd</span><span class=p>(</span><span class=s2>&#34;firstname&#34;</span><span class=p>,</span> <span class=s2>&#34;lastname&#34;</span><span class=p>))</span>
</span></span></code></pre></div><h1 id=cache-vs-persist><code>cache</code> vs <code>persist</code><a hidden class=anchor aria-hidden=true href=#cache-vs-persist>#</a></h1><p>In Spark, you can use the <code>cache</code> and <code>persist</code> functions to store data in memory for faster access. However, there are some important differences between these two functions. The persist function allows you to specify the storage level (e.g., memory only, memory and disk, etc.), while the <code>cache</code> function stores data in the default storage level <code>MEMORY_AND_DISK</code>, first Spark tries to store the dataframe in memory, if there&rsquo;s any excess, it will be stored in disk. It is important to choose the appropriate function based on your use case. The choice of storage level is crucial as it can have a significant impact on the performance of your Spark application.</p><p><strong>Note from the <a href=https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf>Learning Spark: Lightning-Fast Big Data Analysis</a> book:</strong></p><blockquote><p>When you use cache() or persist(), the DataFrame is not fully cached until you invoke an action that goes through every record (e.g., count()). If you use an action like take(1), only one partition will be cached because Catalyst realizes that you do not need to compute all the partitions just to retrieve one record.</p></blockquote><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>Most of the topics I covered in this article were introduced to me in the <a href=https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf>Learning Spark: Lightning-Fast Big Data Analysis</a> book, that I highly recommend to anyone who wants to learn more about Spark, it&rsquo;s a great resource to know more about Spark. I hope you found this article useful, if you have any questions or comments, please <a href=https://twitter.com/_Enamya>reach out</a>. Thanks for reading!</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://aymane11.github.io/tags/apache/>apache</a></li><li><a href=https://aymane11.github.io/tags/spark/>spark</a></li><li><a href=https://aymane11.github.io/tags/gotchas/>gotchas</a></li><li><a href=https://aymane11.github.io/tags/tips/>tips</a></li><li><a href=https://aymane11.github.io/tags/big-data/>big data</a></li><li><a href=https://aymane11.github.io/tags/data/>data</a></li></ul><nav class=paginav><a class=next href=https://aymane11.github.io/posts/avoid-dependecy-hell-in-python-using-pip/><span class=title>Next Page Â»</span><br><span>Avoid dependency hell in Python using pip</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark Gotchas on twitter" href="https://twitter.com/intent/tweet/?text=Apache%20Spark%20Gotchas&url=https%3a%2f%2faymane11.github.io%2fposts%2fapache-spark-gotchas%2f&hashtags=apache%2cspark%2cgotchas%2ctips%2cbigdata%2cdata"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark Gotchas on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2faymane11.github.io%2fposts%2fapache-spark-gotchas%2f&title=Apache%20Spark%20Gotchas&summary=Apache%20Spark%20Gotchas&source=https%3a%2f%2faymane11.github.io%2fposts%2fapache-spark-gotchas%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark Gotchas on reddit" href="https://reddit.com/submit?url=https%3a%2f%2faymane11.github.io%2fposts%2fapache-spark-gotchas%2f&title=Apache%20Spark%20Gotchas"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark Gotchas on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2faymane11.github.io%2fposts%2fapache-spark-gotchas%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark Gotchas on whatsapp" href="https://api.whatsapp.com/send?text=Apache%20Spark%20Gotchas%20-%20https%3a%2f%2faymane11.github.io%2fposts%2fapache-spark-gotchas%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Apache Spark Gotchas on telegram" href="https://telegram.me/share/url?text=Apache%20Spark%20Gotchas&url=https%3a%2f%2faymane11.github.io%2fposts%2fapache-spark-gotchas%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://aymane11.github.io/>enamya | blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerText="copy";function s(){t.innerText="copied!",setTimeout(()=>{t.innerText="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>