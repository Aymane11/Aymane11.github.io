[{"content":"Apache Spark is an open-source, distributed computing system that provides a wide range of tools for data processing, analytics, and machine learning. It\u0026rsquo;s a popular choice for many organizations due to its ability to scale and its support for a wide range of programming languages. However, like any complex system, there are a few gotchas that users should be aware of when working with Spark. In this post, I will cover some of the most common gotchas that I learned while working with Spark, it will also be a sort of summary of some concepts I read about in the Learning Spark: Lightning-Fast Big Data Analysis book (link).\nLazy Evaluation Spark uses a technique called lazy evaluation to optimize the processing of RDDs. This means that transformations on RDDs are not actually executed until an action (e.g., show, count, collect, take) is called on them. This can lead to confusion, as it is not always clear when a transformation will be executed. For example, when timing a transformation, it is important to note that the transformation will not be executed until an action is called, so there is no point in timing the transformation instruction itself.\nval bigDf = ... val transformed = bigDf.filter(...).map(...) // this transformation will not be executed yet (lazy evaluation) transformed.count() // this action will trigger the execution of the transformation and will take more time than the transformation itself Joins Join strategies There are two main strategies for performing joins in Spark: broadcast joins and shuffle joins. The strategy used depends (not only) on the size of the dataframes being joined, and Spark will automatically choose the best strategy. However, it is important to understand how each strategy works to avoid unexpected behavior.\nBroadcast joins Broadcast joins are used when one of the dataframes being joined is small enough to fit in memory (10MB by default). Spark will send a copy of the small dataframe to all nodes in the cluster (hence the name broadast join), and then perform the join locally on each node. This is generally faster than a shuffle join, but it can lead to out of memory errors if the dataframe is too large. Shuffle joins In shuffle joins, Spark will shuffle the dataframes partitions between nodes, to ensure that all data with the same key is on the same node. This is generally slower than a broadcast join because of all the shuffling that occurs, but it can be used to join dataframes of any size. See: Spark Join Strategies - How \u0026amp; What? and The art of joining in Spark\nDuplicate columns after joins An issue that I faced multiple times when working with Spark is having duplicate columns after performing a join between two dataframes having the same join column name.\nval left = Seq((\u0026#34;John\u0026#34;, \u0026#34;Doe\u0026#34;, 29), (\u0026#34;Aymane\u0026#34;, \u0026#34;Boumaaza\u0026#34;, 20), (\u0026#34;Jane\u0026#34;, \u0026#34;Doe\u0026#34;, 29) ).toDF(\u0026#34;firstname\u0026#34;, \u0026#34;lastname\u0026#34;, \u0026#34;age\u0026#34;) // +---------+--------+---+ // |firstname|lastname|age| // +---------+--------+---+ // | John| Doe| 29| // | Aymane|Boumaaza| 20| // | Jane| Doe| 29| // +---------+--------+---+ val right = Seq((\u0026#34;John\u0026#34;), (\u0026#34;Aymane\u0026#34;), (\u0026#34;Jane\u0026#34;)).toDF(\u0026#34;firstname\u0026#34;) // +---------+ // |firstname| // +---------+ // | John| // | Aymane| // | Jane| // +---------+ val joined = left.join(right, left(\u0026#34;firstname\u0026#34;) === right(\u0026#34;firstname\u0026#34;)) // +---------+--------+---+---------+ // |firstname|lastname|age|firstname| // Notice the duplicate column (firstname) // +---------+--------+---+---------+ // | John| Doe| 29| John| // | Aymane|Boumaaza| 20| Aymane| // | Jane| Doe| 29| Jane| // +---------+--------+---+---------+ When performing a join in Spark, use a list/sequence as the by parameter to avoid duplicate columns to ensure that the resulting dataframe only has a single copy of each column.\nval joined = left.join(right, Seq(\u0026#34;firstname\u0026#34;)) // or val joined = left.join(right, \u0026#34;firstname\u0026#34;) if using only one column to join // +---------+--------+---+ // |firstname|lastname|age| // +---------+--------+---+ // | John| Doe| 29| // | Aymane|Boumaaza| 20| // | Jane| Doe| 29| // +---------+--------+---+ See: Databricks - Prevent duplicated columns when joining two DataFrames\nWriting to files Filenames When writing to file in Spark, it is important to note that the output will be written to a directory, not a single file. This is because each partition is written to a separate file, and the files are combined into a single directory. It is important to keep this in mind when specifying the output path, as adding a .csv extension to the path will create a directory with the name filename.csv rather than a single file, therefore there\u0026rsquo;s no need to add the extension (since it\u0026rsquo;s a directory).\nval df = Seq((\u0026#34;John\u0026#34;, \u0026#34;Doe\u0026#34;, \u0026#34;29\u0026#34;), (\u0026#34;Aymane\u0026#34;, \u0026#34;Boumaaza\u0026#34;, \u0026#34;20\u0026#34;), (\u0026#34;Jane\u0026#34;, \u0026#34;Doe\u0026#34;, \u0026#34;29\u0026#34;) ).toDF(\u0026#34;firstname\u0026#34;, \u0026#34;lastname\u0026#34;, \u0026#34;age\u0026#34;) df.write.csv(\u0026#34;/tmp/my_csv.csv\u0026#34;) df.write.parquet(\u0026#34;/tmp/my_parquet.parquet\u0026#34;) /tmp/my_csv.csv and /tmp/my_parquet.parquet will be both directories containing the actual csv and parquet files.\n$ ls -l /tmp drwxr-xr-x 2 root root 4096 Jan 4 20:20 my_csv.csv drwxr-xr-x 2 root root 4096 Jan 4 20:20 my_parquet.parquet Number of partitions When writing to file in Apache Spark, it\u0026rsquo;s important to consider the number of partitions in your data. The more partitions you have, the more files are created. This can be beneficial if you are working with a large dataset and want to parallelize the writing process. However, if you have a small dataset, using too many partitions can lead to unnecessary overhead and slower performance, and vice versa when working with large dataframes and having small number of partitions will result in less files but bigger ones. It is important to carefully consider the size and structure of your dataset when deciding on the number of partitions to use.\nSmall DataFrame Minimize the number of partitions to avoid unnecessary overhead.\nval smallDf = Seq((\u0026#34;John\u0026#34;, \u0026#34;Doe\u0026#34;, \u0026#34;29\u0026#34;), (\u0026#34;Aymane\u0026#34;, \u0026#34;Boumaaza\u0026#34;, \u0026#34;20\u0026#34;), (\u0026#34;Jane\u0026#34;, \u0026#34;Doe\u0026#34;, \u0026#34;29\u0026#34;) ).toDF(\u0026#34;firstname\u0026#34;, \u0026#34;lastname\u0026#34;, \u0026#34;age\u0026#34;) smallDf.rdd.partitions.size // Int = 3 smallDf.write.csv(\u0026#34;/tmp/output\u0026#34;) // /tmp/output will be a directory containing 3 (number of partitions) csv files smallDf.coalesce(1).write.csv(\u0026#34;/tmp/output\u0026#34;) // or repartition(1) // /tmp/output will be a directory containing a single csv file (with other files) Big DataFrame Use a reasonable number of partitions to have manageable output.\nval bigDf = ... bigDf.rdd.partitions.size // Int = 200 bigDf.write.csv(\u0026#34;/tmp/output\u0026#34;) // /tmp/output.csv will be a directory containing 200 (number of partitions) csv files bigDf.coalesce(10).write.csv(\u0026#34;/tmp/output\u0026#34;) // or repartition(10) // /tmp/output.csv will be a directory containing 10 csv files which is more manageable than 200 files Write Options When saving a dataframe to file in Spark, it is important to consider the various options available to you. For example, the header option controls whether the column names are included in the output csv file. By default, this option is set to false, which means that header containing the column names will not be included. It is important to carefully consider the options available to you when writing to file to ensure that the output is in the desired format.\nval df = Seq((\u0026#34;John\u0026#34;, \u0026#34;Doe\u0026#34;, \u0026#34;29\u0026#34;), (\u0026#34;Aymane\u0026#34;, \u0026#34;Boumaaza\u0026#34;, \u0026#34;20\u0026#34;), (\u0026#34;Jane\u0026#34;, \u0026#34;Doe\u0026#34;, \u0026#34;29\u0026#34;) ).toDF(\u0026#34;firstname\u0026#34;, \u0026#34;lastname\u0026#34;, \u0026#34;age\u0026#34;) df.write.csv(\u0026#34;/tmp/output\u0026#34;) // header is false by default df.write.option(\u0026#34;header\u0026#34;,true).option(\u0026#34;delimiter\u0026#34;,\u0026#34;|\u0026#34;).csv(\u0026#34;/tmp/output\u0026#34;) See Spark SQL - Data Source\nPySpark UDFs vs Pandas UDFs When working with User-Defined Functions (UDFs) in PySpark (2.3+), it is often more efficient to use Pandas UDFs (also known as vectorized UDFs) instead of standard Spark UDFs. Because PySpark UDFs require data movement between the JVM and Python, which is expensive, Pandas UDFs allow you to process the data using the power of vectorization and Apache Arrow. This can significantly improve the performance of your Spark application.\nimport pandas as pd from pyspark.sql.types import StringType from pyspark.sql.functions import pandas_udf, udf df = spark.createDataFrame( data=[ (\u0026#34;Aymane\u0026#34;, \u0026#34;Boumaaza\u0026#34;, 20), (\u0026#34;John\u0026#34;, \u0026#34;Doe\u0026#34;, 30), (\u0026#34;Jane\u0026#34;, \u0026#34;Doe\u0026#34;, 40), ], schema=[\u0026#34;firstname\u0026#34;, \u0026#34;lastname\u0026#34;, \u0026#34;age\u0026#34;], ) # Standard UDF @udf(returnType=StringType()) def fullname(firstname, lastname): return f\u0026#34;{firstname.capitalize()} {lastname.upper()}\u0026#34; df.withColumn(\u0026#34;fullname\u0026#34;, fullname(\u0026#34;firstname\u0026#34;, \u0026#34;lastname\u0026#34;)) # Pandas UDF @pandas_udf(StringType()) def fullname_pd(firstname: pd.Series, lastname: pd.Series) -\u0026gt; pd.Series: # You can use any of the pandas APIs here return firstname.str.capitalize() + \u0026#34; \u0026#34; + lastname.str.upper() df.withColumn(\u0026#34;fullname\u0026#34;, fullname_pd(\u0026#34;firstname\u0026#34;, \u0026#34;lastname\u0026#34;)) cache vs persist In Spark, you can use the cache and persist functions to store data in memory for faster access. However, there are some important differences between these two functions. The persist function allows you to specify the storage level (e.g., memory only, memory and disk, etc.), while the cache function stores data in the default storage level MEMORY_AND_DISK, first Spark tries to store the dataframe in memory, if there\u0026rsquo;s any excess, it will be stored in disk. It is important to choose the appropriate function based on your use case. The choice of storage level is crucial as it can have a significant impact on the performance of your Spark application.\nNote from the Learning Spark: Lightning-Fast Big Data Analysis book:\nWhen you use cache() or persist(), the DataFrame is not fully cached until you invoke an action that goes through every record (e.g., count()). If you use an action like take(1), only one partition will be cached because Catalyst realizes that you do not need to compute all the partitions just to retrieve one record.\nConclusion Most of the topics I covered in this article were introduced to me in the Learning Spark: Lightning-Fast Big Data Analysis book, that I highly recommend to anyone who wants to learn more about Spark, it\u0026rsquo;s a great resource to know more about Spark. I hope you found this article useful, if you have any questions or comments, please reach out. Thanks for reading!\n","permalink":"https://aymane11.github.io/posts/apache-spark-gotchas/","summary":"Apache Spark is an open-source, distributed computing system that provides a wide range of tools for data processing, analytics, and machine learning. It\u0026rsquo;s a popular choice for many organizations due to its ability to scale and its support for a wide range of programming languages. However, like any complex system, there are a few gotchas that users should be aware of when working with Spark. In this post, I will cover some of the most common gotchas that I learned while working with Spark, it will also be a sort of summary of some concepts I read about in the Learning Spark: Lightning-Fast Big Data Analysis book (link).","title":"Apache Spark Gotchas"},{"content":"Today I learned about pip config require-virtualenv, this option prevents installing packages unless you are in a virtual environment.\nThere are several ways to set it up:\nVia environment variable:\nexport PIP_REQUIRE_VIRTUALENV=true set PIP_REQUIRE_VIRTUALENV=true # on Windows Via pip config file:\npip config set global.require-virtualenv true Via pip config file (~/.config/pip/pip.conf):\n[global] require-virtualenv = True Once you set it up, you will get an error message if you try to install a package without being in a virtual environment: ","permalink":"https://aymane11.github.io/posts/avoid-dependecy-hell-in-python-using-pip/","summary":"Today I learned about pip config require-virtualenv, this option prevents installing packages unless you are in a virtual environment.\nThere are several ways to set it up:\nVia environment variable:\nexport PIP_REQUIRE_VIRTUALENV=true set PIP_REQUIRE_VIRTUALENV=true # on Windows Via pip config file:\npip config set global.require-virtualenv true Via pip config file (~/.config/pip/pip.conf):\n[global] require-virtualenv = True Once you set it up, you will get an error message if you try to install a package without being in a virtual environment: ","title":"Avoid dependency hell in Python using pip"},{"content":"Introduction Whenever I want to start a new Python project, I used to go for the standard venv included with Python. But, I easily got rid of it, since the folder it creates makes my editor too clumsy.\nAfterwards, I discovered Pipenv and I was very happy with it, until I figured out that it doesn\u0026rsquo;t work well with some packages (e.g. colorama), since it specifies the platform type in the Pipfile.lock file, that I always add to my .gitignore file. Recently I looked for a solution to this cross-platform issue, that\u0026rsquo;s when I found out that people are suggesting using Poetry, a tool which I already heard about, but never took time to give it a chance.\nI finally gave Poetry a chance that day, and from that point, Poetry became my best friend.\nIn my first blog post, I\u0026rsquo;ll take you through my Python Setup.\nPoetry Installing poetry Poetry is a Python packaging and dependency management tool that assists you during the entire development process, from installing dependencies, code packaging, to publishing your code.\nPowershell (Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing).Content | python - Bash curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - Verify that the installation was successful by running poetry --version.\nPoetry lifecycle 1. Project setup poetry new blueprint The above command will ask you for meta infos related to the project (version, authors, license\u0026hellip;), then a new directory will be created with the following content:\nblueprint ├── pyproject.toml # contains dependencies, project metadata and more ├── README.rst ├── blueprint │ └── __init__.py └── tests ├── __init__.py └── test_blueprint.py 2. Add dependencies In order to install a dependency, you need to run the following command:\npoetry add \u0026lt;package\u0026gt; If you\u0026rsquo;d like to add the dependency as a dev dependency, you can use the --dev (-D) flag:\npoetry add -D \u0026lt;dev-package\u0026gt; 3. Export requirements.txt file This command generates a requirements.txt file containing all the dependencies you\u0026rsquo;ve added to your project.\npoetry export -f requirements.txt --output requirements.txt If you want to export the dev dependencies too, you can use the --dev flag:\npoetry export -f requirements.txt --dev --output requirements_dev.txt Poe the Poet Poe is a task runner plugin that runs with Poetry, allowing you to run tasks in Poetry\u0026rsquo;s virtual environment. It can be added to projects using different ways, but I prefer adding it a dev dependency.\npoetry add --dev poethepoet Tasks are defined withing the pyproject.toml file:\n# pyproject.toml [tool.poe.tasks] [tool.poe.tasks.format] help = \u0026#34;Run black on the code base\u0026#34; cmd = \u0026#34;black .\u0026#34; And can be run using the following command:\npoe format # or `poetry run poe format` if outside poetry shell Code Editor My editor of choice is Visual Studio Code and I use it to develop all my projects.\nThis is a list of the Python related extensions I use in my editor:\nPython\u0026rsquo;s official extension: Includes IntelliSense using Pylance, Linting, Jupyter Notebooks, code formatting, refactoring, unit tests \u0026hellip; Python Test Explorer: I\u0026rsquo;ve been struggling with unit tests discovery on Windows, and this extension helps me to run them. Python Environment Manager: This extension gives a full overview on Python environments. autoDocstring: This extension helps me to generate docstrings for my code, all I have to do is type \u0026quot;\u0026quot;\u0026quot; and the extension will generate the docstring for me. Sourcery: An awesome Python refactoring tool. Tabnine and GitHub Copilot: Cause who doesn\u0026rsquo;t like AI writing code for them? Code formatting and linting For code formatting, I use black and for linting, I use flake8 or pylint.\nUnit testing I admit it, I rarely write tests for my code (cause my code never fails 😎), but on serious projects I use pytest with pytest-cov for code coverage, plus pytest-mock mocking fixtures.\nWith poe, I can run tests with coverage using the following command:\npoe test # pyproject.toml [tool.poe.tasks] [tool.poe.tasks.test] help = \u0026#34;Run pytest on the code base\u0026#34; cmd = \u0026#34;pytest -v --cov=src --cov-report=term\u0026#34; Pre-commit hooks Before publishing my code to GitHub, I always have to re-check my code for linting, formatting, security issues\u0026hellip; and I use pre-commit to do that.\nIt can be installed using the following command:\npoetry add --dev pre-commit Here\u0026rsquo;s a sample of the .pre-commit-config.yaml configuration file:\n# .pre-commit-config.yaml repos: - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace - id: check-added-large-files - id: detect-private-key - repo: https://github.com/psf/black rev: 22.1.0 hooks: - id: black language_version: python3.8 - repo: https://gitlab.com/pycqa/flake8 rev: 3.7.9 hooks: - id: flake8 GitHub actions The entire pipeline of my projects is managed by GitHub Actions, here\u0026rsquo;s the CI configuration for my projects:\nname: Check Code on: push: branches: [ main ] # on push to main paths-ignore: - \u0026#39;*.md\u0026#39; # ignore .md files changes pull_request: branches: [ main ] # on pull requests to main paths-ignore: - \u0026#39;*.md\u0026#39; # ignore .md files changes jobs: ci: strategy: max-parallel: 2 matrix: python-version: [3.8, 3.9] poetry-version: [1.1.13] os: [ubuntu-latest, macos-latest, windows-latest] runs-on: ${{ matrix.os }} timeout-minutes: 10 steps: - name: Check out repository code uses: actions/checkout@v2 # Setup Python (faster than using Python container) - name: Setup Python ${{ matrix.python-version }} on ${{ matrix.os }} uses: actions/setup-python@v2 with: python-version: ${{ matrix.python-version }} - name: Install wheel run: python -m pip install wheel - name: Install poetry ${{ matrix.poetry-version }} uses: abatilo/actions-poetry@v2.0.0 with: poetry-version: ${{ matrix.poetry-version }} - name: Cache Poetry virtualenv uses: actions/cache@v2 id: cache with: path: ~/.virtualenvs key: poetry-${{ hashFiles(\u0026#39;**/poetry.lock\u0026#39;) }}-py${{ matrix.python-version }}-os${{ matrix.os }} restore-keys: | poetry-${{ hashFiles(\u0026#39;**/poetry.lock\u0026#39;) }}-py${{ matrix.python-version }}-os${{ matrix.os }} - name: Config Poetry run: | poetry config virtualenvs.in-project false poetry config virtualenvs.path ~/.virtualenvs - name: Install Dependencies using Poetry run: poetry install if: steps.cache.outputs.cache-hit != \u0026#39;true\u0026#39; - name: Check for security issues run: poetry run poe bandit - name: Run test suite run: poetry run poe test Conclusion There are many libraries and tools that can be added to the list such as bandit, autoflake, make, Docker, etc. But I\u0026rsquo;ll let those for another day.\nI made a GitHub repo that I use as a template for my Python projects, give it a try and let me know what you think!\nFinally, I\u0026rsquo;d like to wish you all Ramadan Mubarak! Hope you liked my first blog post!\n","permalink":"https://aymane11.github.io/posts/setup-python-environment/","summary":"Introduction Whenever I want to start a new Python project, I used to go for the standard venv included with Python. But, I easily got rid of it, since the folder it creates makes my editor too clumsy.\nAfterwards, I discovered Pipenv and I was very happy with it, until I figured out that it doesn\u0026rsquo;t work well with some packages (e.g. colorama), since it specifies the platform type in the Pipfile.","title":"Setup Python Environment"},{"content":"Today I learned about Visual Studio Code Snippets, these are code templates that save you the time wasted writing repeating code.\nTo create your own code snippets, select File \u0026gt; Preferences \u0026gt; User Snippets and then select the language you want to create snippets for. You can use the Snippet Generator extension, or this snippet generator webapp (my personal pereference) to help you create the snippets JSON file.\nThis is a list of snippets that I created for Python 🐍.\n","permalink":"https://aymane11.github.io/posts/visual-studio-code-snippets/","summary":"Today I learned about Visual Studio Code Snippets, these are code templates that save you the time wasted writing repeating code.\nTo create your own code snippets, select File \u0026gt; Preferences \u0026gt; User Snippets and then select the language you want to create snippets for. You can use the Snippet Generator extension, or this snippet generator webapp (my personal pereference) to help you create the snippets JSON file.\nThis is a list of snippets that I created for Python 🐍.","title":"Visual Studio Code Snippets"}]